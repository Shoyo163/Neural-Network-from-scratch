{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 30) (114, 30)\n"
     ]
    }
   ],
   "source": [
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target.reshape(-1, 1)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        self.weights = np.random.randn(input_dim,output_dim)*0.01\n",
    "        self.biases = np.zeros((1,output_dim))\n",
    "\n",
    "    def forward(self,X):\n",
    "        self.input = X\n",
    "        self.output = np.dot(X,self.weights) + self.biases\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self,d_loss_pred):\n",
    "        self.d_weights = np.dot(self.input.T,d_loss_pred)\n",
    "        self.d_biases = np.sum(d_loss_pred,axis=0,keepdims=True)\n",
    "        d_output = np.dot(d_loss_pred,self.weights.T)\n",
    "        return d_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self,a):\n",
    "        self.input = a\n",
    "        self.output = np.maximum(0,a)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self,d_output):\n",
    "        d_relu = (self.input > 0)\n",
    "        d_output_relu = d_output * d_relu\n",
    "        return d_output_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def forward(self,logits):\n",
    "        self.logits = logits\n",
    "        self.prediction = 1/(1+np.exp(-logits))\n",
    "        return self.prediction\n",
    "    \n",
    "    def backward(self,d_loss):\n",
    "        d_pred = self.prediction*(1-self.prediction)\n",
    "        d_loss_pred = d_loss * d_pred\n",
    "        return d_loss_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCrossEntropy:\n",
    "    def forward(self,prediction,y):\n",
    "        self.y_pred = prediction\n",
    "        self.y_true = y\n",
    "        self.epsilon = 1e-15\n",
    "        loss = -np.mean(self.y_true * np.log(self.y_pred+self.epsilon) + \n",
    "            (1-self.y_true)* np.log(1-self.y_pred+self.epsilon))\n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        d_loss = (self.y_pred-self.y_true)/((self.y_pred+self.epsilon)*(1-self.y_pred+self.epsilon))\n",
    "        return d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent:\n",
    "    def update(self,layer,learning_rate):\n",
    "        layer.weights = layer.weights - learning_rate*layer.d_weights\n",
    "        layer.biases = layer.biases - learning_rate*layer.d_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.dense1 = Dense(30,16)\n",
    "        self.activation1 = ReLU()\n",
    "        self.dense2 = Dense(16,1)\n",
    "        self.activation2 = Sigmoid()\n",
    "        self.lossfunction = BinaryCrossEntropy()\n",
    "        self.optimizer = GradientDescent()\n",
    "\n",
    "    def forward_prop(self,X):\n",
    "        a = self.dense1.forward(X)\n",
    "        z = self.activation1.forward(a)\n",
    "        logits = self.dense2.forward(z)\n",
    "        prediction = self.activation2.forward(logits)\n",
    "        return prediction\n",
    "    \n",
    "    def backward_prop(self,learning_rate):\n",
    "        d_loss = self.lossfunction.backward()\n",
    "        d_loss_pred = self.activation2.backward(d_loss)\n",
    "        d_output = self.dense2.backward(d_loss_pred)\n",
    "        self.optimizer.update(self.dense2,learning_rate)\n",
    "        d_output_relu = self.activation1.backward(d_output)\n",
    "        f_output = self.dense1.backward(d_output_relu)\n",
    "        self.optimizer.update(self.dense1,learning_rate)\n",
    "\n",
    "    def train(self,X_train,y_train,epochs,learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward_prop(X_train)\n",
    "            y_labels = (y_pred > 0.5)\n",
    "            acc = np.mean(y_labels == y_train)\n",
    "            loss = self.lossfunction.forward(y_pred,y_train)\n",
    "            if (epoch+1)%100==0:\n",
    "                print(f\"In Epoch: {epoch+1}, Loss: {loss}, Training Accuracy: {acc*100}\")\n",
    "            self.backward_prop(learning_rate)\n",
    "\n",
    "    def predict(self,X):\n",
    "        y_pred = self.forward_prop(X)\n",
    "        y_labels = (y_pred > 0.5)\n",
    "        return y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Epoch: 100, Loss: 0.008780733757753819, Training Accuracy: 99.78021978021978\n",
      "In Epoch: 200, Loss: 0.003508039519040072, Training Accuracy: 100.0\n",
      "In Epoch: 300, Loss: 0.0018835949828411066, Training Accuracy: 100.0\n",
      "In Epoch: 400, Loss: 0.0012009960342784236, Training Accuracy: 100.0\n",
      "In Epoch: 500, Loss: 0.0008487139742320662, Training Accuracy: 100.0\n",
      "In Epoch: 600, Loss: 0.0006428509004818844, Training Accuracy: 100.0\n",
      "In Epoch: 700, Loss: 0.000511612847366385, Training Accuracy: 100.0\n",
      "In Epoch: 800, Loss: 0.00042021420737300784, Training Accuracy: 100.0\n",
      "In Epoch: 900, Loss: 0.00035501065151014114, Training Accuracy: 100.0\n",
      "In Epoch: 1000, Loss: 0.0003057988845645052, Training Accuracy: 100.0\n"
     ]
    }
   ],
   "source": [
    "nn.train(X_train,y_train,epochs=1000,learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  95.6140350877193\n"
     ]
    }
   ],
   "source": [
    "test_preds = nn.predict(X_test)\n",
    "test_labels = (test_preds > 0.5)\n",
    "acc = np.mean(test_labels == y_test)\n",
    "print(\"Test Accuracy: \",acc*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
